{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a382c8d2",
   "metadata": {},
   "source": [
    "# Gaussian Process Kinematics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20807d2",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baa1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc447a0b",
   "metadata": {},
   "source": [
    "We will start from a pre-generated dataset for this lab. This data comes from a simple\n",
    "kinematics simulation that follows these equations:\n",
    "\n",
    "$$\n",
    "X_t = t \\cdot \\sin\\left(\\frac{2\\pi t}{P}\\right) + W_t\n",
    "$$\n",
    "\n",
    "where $T$ is a parameter of our system and $W_t$ is white noise with\n",
    "\n",
    "$$\n",
    "W_t \\sim \\mathcal{N}(0, \\sigma_w^2)\n",
    "$$\n",
    "\n",
    "This equation is the product of a periodic function and a linear function, so it will be\n",
    "interesting to see how different kernel assumptions for our Gaussian Process adapt to\n",
    "the data. Our goal is to infer the true trajectory of the object from the noisy data.\n",
    "\n",
    "We will generate a dataset where $P = 0.2$ and $\\sigma_w = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63016283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate the kinematics dataset.\n",
    "x = np.linspace(start=0, stop=10, num=1000).reshape(-1, 1)\n",
    "f_true = 0.5 * np.squeeze(x * np.sin(x / 0.2))\n",
    "\n",
    "rng = np.random.seed(2)\n",
    "obs_indices = np.random.choice(np.arange(f_true.size), size=10, replace=False)\n",
    "x_obs, f_obs = x[obs_indices], f_true[obs_indices]\n",
    "y_obs = f_obs + np.random.normal(loc=0.0, scale=0.5, size=f_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d7bb5",
   "metadata": {},
   "source": [
    "### Part I: Implementing the Base Gaussian Process Class\n",
    "\n",
    "To compare different kernels you will need to implement a GP class that:\n",
    "\n",
    "1. Implement the calculation of the K matrix.\n",
    "2. Store the data and the inverse K matrix for prediction.\n",
    "3. Implement the mean and covariance prediction functions.\n",
    "4. Implement calculation of likelihood of observations given kernel parameters.\n",
    "\n",
    "We will assume that\n",
    "\n",
    "$$\n",
    "\\mu(t) = 0\n",
    "$$\n",
    "\n",
    "for our GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a84f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    \"\"\"Class that implements a Gaussian Process.\n",
    "\n",
    "    Args:\n",
    "        kernel_function: Function for calculating kappa(x,x').\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_function: Any):\n",
    "        \"\"\"Initialize our class.\"\"\"\n",
    "        # Save the vectorized kernel function. Your kernel function\n",
    "        # will take as input two scalar, but the vectorized function needs\n",
    "        # to take in two arrays, one of length $m$ and one of length $n$,\n",
    "        # and return a mxn matrix. We use one call of numpy.vectorize and take\n",
    "        # advantage of the signature options.\n",
    "        self._kernel_function = (\n",
    "            np.vectorize(kernel_function,\n",
    "                         signature='(),(m)->()'\n",
    "                        )\n",
    "        )\n",
    "\n",
    "        # Initialize to None so we know to predict the prior to start.\n",
    "        self.k_matrix_inv = None\n",
    "        self.observed_x = None\n",
    "        self.observed_y = None\n",
    "\n",
    "    def calc_k_matrix(self, x_rows: np.ndarray, x_cols: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate the K matrix at the given positions.\n",
    "\n",
    "        Args:\n",
    "            x_rows: X-position for the rows.\n",
    "            x_cols: X-position for the columns.\n",
    "\n",
    "        Returns:\n",
    "            K(x_rows,x_cols) matrix.\n",
    "        \"\"\"\n",
    "        #  All this function needs to do is call our vectorized kernel function\n",
    "        return self._kernel_function(x_rows, x_cols.T)\n",
    "\n",
    "\n",
    "    def set_observations(self, observed_x: np.ndarray, observed_y: np.ndarray,\n",
    "                         sigma_w: float):\n",
    "        \"\"\"Store the x and y that have been observed.\n",
    "\n",
    "        Args:\n",
    "            observed_x: Observed input points x.\n",
    "            observed_y: Observed (noisy) outputs y.\n",
    "            sigma_w: Observational noise standard deviation.\n",
    "\n",
    "        Notes:\n",
    "            Modifies internal variables.\n",
    "        \"\"\"\n",
    "        # Save the observations.\n",
    "        self.observed_x = observed_x\n",
    "        self.observed_y = observed_y\n",
    "\n",
    "        k_matrix = self.calc_k_matrix(observed_x, observed_x)\n",
    "\n",
    "        # Save the K matrix inverse with the noise included.\n",
    "        self.k_matrix_inv = np.linalg.inv(k_matrix + sigma_w ** 2 * np.eye(len(observed_x)))\n",
    "\n",
    "    def predict(self, predict_x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Predict the mean and covariance at given points.\n",
    "\n",
    "        Args:\n",
    "            predict_x: X-positions at which to make predictions.\n",
    "\n",
    "        Returns:\n",
    "            Mean and covariance matrix at the requested points.\n",
    "\n",
    "        Notes:\n",
    "            If no observations have been set, this will return the prior.\n",
    "        \"\"\"\n",
    "        # Calculate the k_star_star matrix.\n",
    "        k_star_star_matrix = self.calc_k_matrix(predict_x, predict_x)\n",
    "\n",
    "        # Start with the prior predictions\n",
    "        mean_pred = np.zeros(len(predict_x))\n",
    "        cov_pred = k_star_star_matrix.copy()\n",
    "\n",
    "        # Correct for observed data if it exists.\n",
    "        if self.observed_x is not None:\n",
    "            # Calculate the k_star matrix. Use the saved k_matrix_inv for speed\n",
    "            # of computation.\n",
    "            k_star_matrix = self.calc_k_matrix(self.observed_x, predict_x)\n",
    "\n",
    "            # Update mean and covariance prediction.\n",
    "            mean_pred += k_star_matrix.T @ self.k_matrix_inv @ self.observed_y\n",
    "            cov_pred -=  k_star_matrix.T @ self.k_matrix_inv @ k_star_matrix\n",
    "\n",
    "        return mean_pred, cov_pred\n",
    "\n",
    "    def sample_f(self, predict_x: np.ndarray, n_samples: Optional[int] = 1) -> np.ndarray:\n",
    "        \"\"\"Return samples of the function space at the desired points.\n",
    "\n",
    "        Args:\n",
    "            predict_x: X-positions at which to make predictions.\n",
    "            n_samples: Number of function samples to draw.\n",
    "\n",
    "        Returns:\n",
    "            Samples of the function space outputs.\n",
    "        \"\"\"\n",
    "        # Get the mean and covariance prediction from our function.\n",
    "        mean_pred, cov_pred = self.predict(predict_x)\n",
    "\n",
    "        # Draw samples.\n",
    "        f_samples = np.zeros((n_samples, len(predict_x)))\n",
    "        for i in range(len(f_samples)):\n",
    "            f_samples[i] = multivariate_normal.rvs(mean=mean_pred, cov=cov_pred)\n",
    "\n",
    "        return f_samples\n",
    "\n",
    "    def log_likelihood(self) -> float:\n",
    "        \"\"\"Return the log likelihood of the data for the given kernel.\n",
    "\n",
    "        Returns:\n",
    "            Log likelihood of the data.\n",
    "        \"\"\"\n",
    "        # Implement the log likelihood of the data.\n",
    "        n = len(self.observed_y)\n",
    "        k_matrix = self.calc_k_matrix(self.observed_x, self.observed_x)\n",
    "        sigma = k_matrix + 0.5 ** 2 * np.eye(n)\n",
    "        sign, logdet = np.linalg.slogdet(sigma)\n",
    "        quad_form = self.observed_y.T @ np.linalg.inv(sigma) @ self.observed_y\n",
    "        log_likelihood_calc = -0.5 * quad_form - 0.5 * logdet - 0.5 * n * np.log(2 * np.pi)\n",
    "\n",
    "        return log_likelihood_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f24d9f",
   "metadata": {},
   "source": [
    "Let’s test our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc915ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\n(shapes (4, 1), (4, 4) mismatch)\n ACTUAL: array([[0.000419],\n       [0.035674],\n       [0.035674],\n       [0.000419]])\n DESIRED: array([[1.      , 0.573753, 0.108368, 0.006738],\n       [0.573753, 1.      , 0.573753, 0.108368],\n       [0.108368, 0.573753, 1.      , 0.573753],\n       [0.006738, 0.108368, 0.573753, 1.      ]])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Test the K matrix calculation.\u001b[39;00m\n\u001b[32m     10\u001b[39m k_matrix = gp_test.calc_k_matrix(x_test, x_test)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_array_almost_equal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.573753\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.108368\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.006738\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.573753\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.573753\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.108368\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.108368\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.573753\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.573753\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.006738\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.108368\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.573753\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Test the predictions without observations.\u001b[39;00m\n\u001b[32m     20\u001b[39m mean_test, cov_test = gp_test.predict(x_test)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/numpy/testing/_private/utils.py:813\u001b[39m, in \u001b[36massert_array_compare\u001b[39m\u001b[34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\u001b[39m\n\u001b[32m    806\u001b[39m         reason = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m(dtypes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m mismatch)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    807\u001b[39m     msg = build_err_msg([x, y],\n\u001b[32m    808\u001b[39m                         err_msg\n\u001b[32m    809\u001b[39m                         + reason,\n\u001b[32m    810\u001b[39m                         verbose=verbose, header=header,\n\u001b[32m    811\u001b[39m                         names=names,\n\u001b[32m    812\u001b[39m                         precision=precision)\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[32m    815\u001b[39m flagged = np.bool(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isnumber(x) \u001b[38;5;129;01mand\u001b[39;00m isnumber(y):\n",
      "\u001b[31mAssertionError\u001b[39m: \nArrays are not almost equal to 6 decimals\n\n(shapes (4, 1), (4, 4) mismatch)\n ACTUAL: array([[0.000419],\n       [0.035674],\n       [0.035674],\n       [0.000419]])\n DESIRED: array([[1.      , 0.573753, 0.108368, 0.006738],\n       [0.573753, 1.      , 0.573753, 0.108368],\n       [0.108368, 0.573753, 1.      , 0.573753],\n       [0.006738, 0.108368, 0.573753, 1.      ]])"
     ]
    }
   ],
   "source": [
    "# A few tests for our GP functions.\n",
    "def test_kernel_function(x, x_prime):\n",
    "    return np.exp(-np.sum(np.square(x - x_prime))/20)\n",
    "\n",
    "gp_test = GaussianProcess(test_kernel_function)\n",
    "\n",
    "x_test = np.linspace(0,10,4).reshape(-1, 1)\n",
    "\n",
    "# Test the K matrix calculation.\n",
    "k_matrix = gp_test.calc_k_matrix(x_test, x_test)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    k_matrix,\n",
    "    [[1.0, 0.573753, 0.108368, 0.006738],\n",
    "     [0.573753, 1.0, 0.573753, 0.108368],\n",
    "     [0.108368, 0.573753, 1.0, 0.573753],\n",
    "     [0.006738, 0.108368, 0.573753, 1.0]]\n",
    ")\n",
    "\n",
    "# Test the predictions without observations.\n",
    "mean_test, cov_test = gp_test.predict(x_test)\n",
    "np.testing.assert_array_almost_equal(mean_test, np.zeros(4))\n",
    "np.testing.assert_array_almost_equal(\n",
    "    cov_test,\n",
    "    [[1.0, 0.573753, 0.108368, 0.006738],\n",
    "     [0.573753, 1.0, 0.573753, 0.108368],\n",
    "     [0.108368, 0.573753, 1.0, 0.573753],\n",
    "     [0.006738, 0.108368, 0.573753, 1.0]]\n",
    ")\n",
    "\n",
    "# Test the predictions with observations\n",
    "x_obs_test = np.linspace(0,10,4).reshape(-1, 1)\n",
    "y_obs_test = np.linspace(5,50,4)\n",
    "gp_test.set_observations(x_obs_test, y_obs_test, sigma_w=0.3)\n",
    "mean_test, cov_test = gp_test.predict(x_test)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    mean_test,\n",
    "    np.array([5.346965, 18.628744, 34.861521, 46.078638])\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    cov_test,\n",
    "    [[ 0.078941,  0.007533, -0.003469,  0.001145],\n",
    "     [ 0.007533,  0.073929,  0.009536, -0.003469],\n",
    "     [-0.003469,  0.009536,  0.073929,  0.007533],\n",
    "     [ 0.001145, -0.003469,  0.007533,  0.078941]]\n",
    ")\n",
    "\n",
    "# Test the log likelihood calculation.\n",
    "np.testing.assert_almost_equal(\n",
    "    gp_test.log_likelihood(),\n",
    "    -1262.211359395657\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0a5d8",
   "metadata": {},
   "source": [
    "### Part II: Comparing Kernels for our GP.\n",
    "\n",
    "Let’s compare how our different kernels impact our predictions. We can also test how different\n",
    "kernel values are favored by the model. We will:\n",
    "\n",
    "1. Compare the prediction of a squared exponential and a cosine kernel.\n",
    "2. See how different choices of the period for our cosine kernel impact the likelihood of the\n",
    "   observed data.\n",
    "\n",
    "The squared exponential kernel is given by:\n",
    "\n",
    "$$\n",
    "\\kappa(x, x') = A \\cdot \\exp\\left( -\\frac{(x - x')^2}{2l^2} \\right)\n",
    "$$\n",
    "\n",
    "The cosine kernel is given by:\n",
    "\n",
    "$$\n",
    "\\kappa(x, x') = A \\cdot \\cos\\left( \\frac{|x - x'|}{P} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to implement our two kernels.\n",
    "def cosine_kernel(x: float, x_prime: float, period: float, amplitude: float) -> float:\n",
    "    \"\"\"Cosine kernel function.\n",
    "\n",
    "    Args:\n",
    "        x: First x-position at which to calculate the kernel.\n",
    "        x_prime: Second x-position at which to calculate the kernel.\n",
    "        period: Period of the cosine kernel.\n",
    "        amplitude: Amplitude of the cosine kernel.\n",
    "\n",
    "    Returns:\n",
    "        Kernel function value.\n",
    "    \"\"\"\n",
    "    # Implement cosine kernel function.\n",
    "    return amplitude * np.cos(np.abs(x - x_prime) / period)\n",
    "\n",
    "def squared_exp_kernel(x: float, x_prime: float, length_scale: float, amplitude: float) -> float:\n",
    "    \"\"\"Cosine kernel function.\n",
    "\n",
    "    Args:\n",
    "        x: First x-position at which to calculate the kernel.\n",
    "        x_prime: Second x-position at which to calculate the kernel.\n",
    "        length_scale: Length scale of the squared exponential kernel.\n",
    "        amplitude: Amplitude of the squared exponential kernel.\n",
    "\n",
    "    Returns:\n",
    "        Kernel function value.\n",
    "    \"\"\"\n",
    "    # Implement cosine kernel function.\n",
    "    return amplitude * np.exp(-np.square(x - x_prime) / (2 * length_scale ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300b7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's visualize the squared exponential kernel.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Use functools.partial to set our kernel.\n",
    "length_scale = 1.0\n",
    "amplitude = 2.0\n",
    "kernel_function = functools.partial(squared_exp_kernel, length_scale=length_scale, amplitude=amplitude)\n",
    "gp = GaussianProcess(kernel_function)\n",
    "sigma_w = 0.5\n",
    "\n",
    "gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "plt.plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "plt.plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "plt.plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "plt.fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std,\n",
    "                 color = '#99d8c9', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('f(Time)')\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f85106",
   "metadata": {},
   "source": [
    "**Question:** Why are these predictions doing poorly? Limit your answer to the form of the kernel and not the specific parameters values.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's visualize the cosine kernel.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Use functools.partial to set our kernel.\n",
    "period = 0.2\n",
    "amplitude = 2.0\n",
    "kernel_function = functools.partial(cosine_kernel, period=period, amplitude=amplitude)\n",
    "gp = GaussianProcess(kernel_function)\n",
    "sigma_w = 0.5\n",
    "\n",
    "gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "plt.plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "plt.plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "plt.plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "plt.fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std,\n",
    "                 color = '#99d8c9', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('f(Time)')\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b03604",
   "metadata": {},
   "source": [
    "**Question:** How are these new predictions better than the previous? Why are the predictions still limited? Limit your answer to the form of the kernel and not the specific parameters values.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e914964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we combine our two kernels together?\n",
    "def combined_kernel(x: float, x_prime: float, period_cosine: float, amplitude_cosine: float,\n",
    "                    length_scale: float, amplitude_se: float) -> float:\n",
    "    \"\"\"Sum of cosine and squared exponential kernel.\n",
    "\n",
    "    Args:\n",
    "        x: First x-position at which to calculate the kernel.\n",
    "        x_prime: Second x-position at which to calculate the kernel.\n",
    "        period_cosine: Period of the cosine kernel.\n",
    "        amplitude_cosine: Amplitude of the cosine kernel.\n",
    "        length_scale: Length scale of the squared exponential kernel.\n",
    "        amplitude_se: Amplitude of the squared exponential kernel.\n",
    "\n",
    "    Returns:\n",
    "        Kernel function value.\n",
    "    \"\"\"\n",
    "    # Implement the combination.\n",
    "    return cosine_kernel(x, x_prime, period_cosine, amplitude_cosine) + squared_exp_kernel(x, x_prime, length_scale, amplitude_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba4878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's visualize the combined kernel.\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Use functools.partial to set our kernel.\n",
    "period_cosine = 0.2\n",
    "amplitude_cosine = 2.0\n",
    "length_scale = 0.2\n",
    "amplitude_se = 2.0\n",
    "kernel_function = functools.partial(\n",
    "     combined_kernel,\n",
    "     period_cosine=period_cosine,\n",
    "     amplitude_cosine=amplitude_cosine,\n",
    "     length_scale=length_scale,\n",
    "     amplitude_se=amplitude_se\n",
    " )\n",
    "gp = GaussianProcess(kernel_function)\n",
    "sigma_w = 0.5\n",
    "\n",
    "gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "plt.plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "plt.plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "plt.plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "plt.fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std,\n",
    "                 color = '#99d8c9', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('f(Time)')\n",
    "plt.ylim([-10,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06547f04",
   "metadata": {},
   "source": [
    "**Question:** How are these new predictions better than the previous? Why might this be the case? Limit your answer to the form of the kernel and not the specific parameters values.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c27e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will compare the quality of different period choices.\n",
    "periods = np.linspace(0.1,0.3,1000)\n",
    "log_likelihoods = np.zeros(periods.shape)\n",
    "\n",
    "# Let's start by plotting few length scales.\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5), dpi=100)\n",
    "\n",
    "for i, period in enumerate([0.1, 0.2, 0.3]):\n",
    "    # Use the looped period parameter to initialize the kernel function.\n",
    "    amplitude_cosine = 2.0\n",
    "    length_scale = 0.2\n",
    "    amplitude_se = 2.0\n",
    "    kernel_function = functools.partial(\n",
    "        combined_kernel,\n",
    "        period_cosine=period,\n",
    "        amplitude_cosine=amplitude_cosine,\n",
    "        length_scale=length_scale,\n",
    "        amplitude_se=amplitude_se\n",
    "    )\n",
    "    gp = GaussianProcess(kernel_function)\n",
    "    gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "    y_pred_mean, y_pred_cov = gp.predict(x)\n",
    "    y_pred_std = np.sqrt(np.diag(y_pred_cov))\n",
    "    ax[i].plot(x, y_pred_mean, color = '#99d8c9', label = 'GP Posterior Distribution')\n",
    "    ax[i].plot(x_obs, y_obs, 'x', color = 'k', label = 'Observed Data', markersize=7)\n",
    "    ax[i].plot(x, f_true, '--', color = 'k', label = 'True Generative Function')\n",
    "    ax[i].fill_between(x[:,0], y_pred_mean - 2 * y_pred_std, y_pred_mean + 2 * y_pred_std,\n",
    "                       color = '#99d8c9', alpha=0.5)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Time')\n",
    "    ax[i].set_ylabel('f(Time)')\n",
    "    ax[i].set_ylim([-10,10])\n",
    "    ax[i].set_title(f'Period = {period}')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# For each length scale, calculate the log likelihood.\n",
    "for i, period in enumerate(periods):\n",
    "    # Use the looped period parameter to initialize the kernel function.\n",
    "    amplitude_cosine = 2.0\n",
    "    length_scale = 0.2\n",
    "    amplitude_se = 2.0\n",
    "    kernel_function = functools.partial(\n",
    "        combined_kernel,\n",
    "        period_cosine=period,\n",
    "        amplitude_cosine=amplitude_cosine,\n",
    "        length_scale=length_scale,\n",
    "        amplitude_se=amplitude_se\n",
    "    )\n",
    "    gp = GaussianProcess(kernel_function)\n",
    "    gp.set_observations(x_obs, y_obs, sigma_w)\n",
    "    log_likelihoods[i] = gp.log_likelihood()\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(periods, log_likelihoods)\n",
    "# plt.axvline(0.2, c='k')\n",
    "plt.xlabel(r'Period $P$', fontsize=20)\n",
    "plt.ylabel(r'$\\log p(\\mathbf{y}|\\mathbf{X},P)$', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c956b",
   "metadata": {},
   "source": [
    "**Question:** What value of the period parameter maximizes the data likelihood? Does this make sense? How would you describe the predictions from p=0.1? Too complex or to too simple? Does this make sense?\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
